import streamlit as st

from langchain.memory import ConversationBufferMemory
from utils import qa_agent


st.title("ğŸ“ƒå…‹éš†GPT æ™ºèƒ½PFDé—®ç­”å™¨ğŸ’¡")

with st.sidebar:
    openai_api_key = st.text_input("è¯·è¾“å…¥OpenAI APIå¯†é’¥ï¼š", type="password")
    st.markdown("[è·å–OpneAI APIå¯†é’¥](https://platform.openai.com/account/api-keys)")

#åªæœ‰å½“å¯¹è¯æ¡†é‡Œæ²¡æœ‰memoryé”®çš„æ—¶å€™æ‰åˆå§‹åŒ–è®°å¿†
if "memory" not in st.session_state:
    # åˆ›å»ºè®°å¿†çš„å®ä¾‹
    st.session_state["memory"] = ConversationBufferMemory(
        return_messages=True,
        memory_key="chat_history",
        output_key="answer"
    )

uploaded_file = st.file_uploader("ä¸Šä¼ ä½ çš„PDFæ–‡ä»¶ï¼š",type="pdf")
question = st.text_input("å¯¹PDFçš„å†…å®¹è¿›è¡Œæé—®", disabled=not uploaded_file)

if uploaded_file and question and not openai_api_key:
    st.info("è¯·è¾“å…¥ä½ çš„OpenAI APIå¯†é’¥")

if uploaded_file and question and openai_api_key:
    #æ·»åŠ åŠ è½½ç»„ä»¶
    with st.spinner("NETA CPTæ­£åœ¨æ€è€ƒä¸­ï¼Œè¯·ç¨ç­‰..."):
        response = qa_agent(openai_api_key, st.session_state["memory"],
                            uploaded_file, question)
    st.write("### ç­”æ¡ˆ")
    st.write(response["answer"])
    #æŠŠå†å²å¯¹è¯å‚¨å­˜åˆ°å¯¹è¯çŠ¶æ€é‡Œ
    st.session_state["chat_history"]= response["chat_history"]

if "chat_history" in st.session_state:
    #å†å²æ¶ˆæ¯çš„æŠ˜å ç»„ä»¶
    with st.expander("å†å²æ¶ˆæ¯"):
        for i in range(0, len(st.session_state["chat_history"]),2):
            human_message = st.session_state["chat_history"][i]
            ai_message = st.session_state["chat_history"][i+1]
            st.write(human_message.content)
            st.write(ai_message.content)
            if i < len(st.session_state["chat_history"]) - 2:
                st.divider()

